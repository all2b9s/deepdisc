{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e155eb64-94e2-4129-a798-cfa149839eb2",
   "metadata": {},
   "source": [
    "## Demo with BTK data\n",
    "\n",
    "This walkthrough uses sims generated from the Blending Toolkit (Mendoza et al. 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import LazyConfig\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import SimpleTrainer\n",
    "from detectron2.engine import HookBase\n",
    "from typing import Dict, List, Optional\n",
    "import detectron2.solver as solver\n",
    "import detectron2.modeling as modeler\n",
    "import detectron2.data as data\n",
    "import detectron2.data.transforms as T\n",
    "import detectron2.checkpoint as checkpointer\n",
    "from detectron2.data import detection_utils as utils\n",
    "import weakref\n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import deepdisc.astrodet.astrodet as toolkit\n",
    "from deepdisc.astrodet import detectron as detectron_addons\n",
    "\n",
    "from deepdisc.data_format.file_io import DDLoader\n",
    "from deepdisc.data_format.annotation_functions.annotate_hsc import annotate_hsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b27c12-e635-43d1-82ee-c85115bacfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path to where you've downloaded the data\n",
    "dirpath = \n",
    "#Change to where you want to output the model\n",
    "output_dir = './'\n",
    "\n",
    "dataset_names = [\"train\", \"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261cf5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training data\n",
    "\n",
    "\n",
    "We have formatted some BTK data for this demo notebook. You can find it here \n",
    "\n",
    "The network needs both images and metadata to train.  The metadata consists of a few image properties along with \"annotations\" for each object.  Annotations are dictionaries that contain information about the objects' bounding boxes, segmenation masks, class, and any other information you want to use.  \n",
    "\n",
    "In this demo, we use data that already has annotations generated.  The preprocessing tutorial in the docs walks through using the scarlet deblender on an image dataset to produce annotations.  You can find examples of the annotation generation in the deepdisc.data_format.annotation_functions module.  For a custom dataset, you may need to change the annotation functions to properly format your metadata.  \n",
    "\n",
    "Once created, you can save the metadata into a json file to load in later.  We will use a saved json file of metadata below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e340e-7339-496f-82b9-cc774fde238d",
   "metadata": {},
   "source": [
    "The flexible `DDLoader` class can be used create metadata from output files, or load in saved metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2f969-1df7-4ab4-8fbf-5b8d4e2176c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DDLoader class, which will just be used to load existing files\n",
    "json_loader = DDLoader()\n",
    "\n",
    "\n",
    "dataset_dicts = {}\n",
    "\n",
    "for i, d in enumerate(dataset_names):\n",
    "    print(f\"Loading {d}\")\n",
    "    filenames_dir = os.path.join(dirpath, d)\n",
    "    filepath = filenames_dir + \".json\"\n",
    "    dataset_dicts[d] = json_loader.load_coco_json_file(filepath).get_dataset()\n",
    "    \n",
    "    \n",
    "    # Lets you rerun this cell without raising an error\n",
    "    if f\"astro_{d}\" in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(f\"astro_{d}\")\n",
    "    \n",
    "    DatasetCatalog.register(f\"astro_{d}\", lambda: json_loader.load_coco_json_file(filepath).get_dataset())\n",
    "    MetadataCatalog.get(f\"astro_{d}\").set(thing_classes=[\"galaxy\"])\n",
    "\n",
    "astrotrain_metadata = MetadataCatalog.get(f\"astro_train\")\n",
    "astrotest_metadata = MetadataCatalog.get(f\"astro_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41802f0c-05a1-4b10-81f2-245296ca3a85",
   "metadata": {},
   "source": [
    "We registered the dataset, which links a string name (\"astro_train/test\") to the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6ec6b-fcb4-40d2-8510-06916fb75697",
   "metadata": {},
   "source": [
    "### A note on classes\n",
    "\n",
    "In this demo, we assume one class for all objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7744f-6bf6-4b0b-bf98-75fac74eccd7",
   "metadata": {},
   "source": [
    "### Dataloading and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4eadb-2af4-4199-b7b2-1f8a580e031a",
   "metadata": {
    "tags": []
   },
   "source": [
    "During training, the network will randomly sample the metadata.  Each entry in the metadata dictionary correpsonds to a single image.  To save memory, the image itself is read in on-the-fly using a filepath stored in the metadata and some function to read from the disk.\n",
    "\n",
    "\n",
    "All of this is wrapped into the `DictMapper` class.  The `DictMapper` has three key pieces\n",
    "\n",
    "1. ImageReader: This class will load the image from the disk, and apply any scalings (Lupton, custom, etc.)\n",
    "2. key_mapper: This will take the metadata and return a filepath (or paths) to an image.  Most of the time it will just return the filepath already stored in the metadata, but if you move the data or have multiple images per file, it can be used to return the correct filepath(s).\n",
    "3. augmentations: These are custom data augmentations that will be applied during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4e6c1-ec8b-41d0-bee6-d6e54cd227a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "from deepdisc.data_format.image_readers import DC2ImageReader\n",
    "reader = DC2ImageReader(norm=\"lupton\",stretch=6000,     # ImageReader class to read the image from the disk and apply contrast scalings\n",
    "                        Q=10, bandlist=[2,1,0])   \n",
    "#2\n",
    "def key_mapper(dataset_dict):\n",
    "    key = os.path.join(dirpath,'img_'+dataset_dict['file_name'].split('_')[1]+'.npy')\n",
    "    return key\n",
    "\n",
    "#3\n",
    "from deepdisc.data_format.augment_image import dc2_train_augs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30ca44-a955-420f-aa36-728a48097839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdisc.model.loaders as loaders\n",
    "tm = loaders.DictMapper                                # DictMapper class to read in the dictionaries and reformat them for the model\n",
    "mapper = tm(reader, key_mapper, dc2_train_augs).map_data   # Map (reformat) the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0491726-1231-4399-9f67-cd360abfe205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "dictionary = iter(dataset_dicts[\"train\"])\n",
    "d = next(dictionary)\n",
    "img = reader(key_mapper(d))\n",
    "\n",
    "visualizer = Visualizer(img, metadata=astrotrain_metadata, scale=5)\n",
    "# Get the ground truth boxes\n",
    "gt_boxes = np.array([a[\"bbox\"] for a in d[\"annotations\"]])\n",
    "# Convert to the mode visualizer expects\n",
    "gt_boxes = BoxMode.convert(gt_boxes, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "out = visualizer.overlay_instances(boxes=gt_boxes)\n",
    "ax.imshow(out.get_image())\n",
    "ax.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f45c7-64f6-469f-be43-52aa13c20755",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b37de-172b-4594-9f5c-b47a8d449aa1",
   "metadata": {},
   "source": [
    "Data augmentations can be applied during training.  We have some pre-defined basic spatial augmentaions like flips and 90 degree rotations, as well as cropping. Any function can be applied as an augmentation.  Check out the `data_format.augment_image` module for examples.  Also see the detectron2 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10 * 2, 10))\n",
    "\n",
    "dictionary = iter(dataset_dicts[\"train\"])\n",
    "d = next(dictionary)\n",
    "img = reader(key_mapper(d))\n",
    "\n",
    "visualizer = Visualizer(img, metadata=astrotrain_metadata, scale=5)\n",
    "# Get the ground truth boxes\n",
    "gt_boxes = np.array([a[\"bbox\"] for a in d[\"annotations\"]])\n",
    "# Convert to the mode visualizer expects\n",
    "gt_boxes = BoxMode.convert(gt_boxes, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "out = visualizer.overlay_instances(boxes=gt_boxes)\n",
    "axs[0].imshow(out.get_image())\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "aug_d = mapper(d)\n",
    "img_aug = aug_d[\"image_shaped\"]\n",
    "visualizer = Visualizer(img_aug, metadata=astrotrain_metadata, scale=5)\n",
    "print(img_aug.shape)\n",
    "# Convert to the mode visualizer expects\n",
    "out = visualizer.overlay_instances(boxes=aug_d[\"instances\"].gt_boxes)\n",
    "axs[1].imshow(out.get_image())\n",
    "axs[1].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-replication",
   "metadata": {},
   "source": [
    "### Prepare For Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9146db4-7391-4572-a269-14bbf3877385",
   "metadata": {},
   "source": [
    "We prepare for training by intializing a config object.  Then we can take the intial weights from the pre-trained models in the model zoo.\n",
    "This setup is for demo purposes, so it does not follow a full training schedule.\n",
    "\n",
    "We train by setting up a trainer object.  This will take a pytorch nn model, a loader (which is just the mapper from above with some detectron2 wrappings), an optimizer which sets the learning rate and scheduler, and any additional operations during training which are called \"hooks\".  Below, we use a hook to save the model every few iterations, and a hook that employs the learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef54d6-b3df-45f0-8ba0-29a86a74d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdisc.training.trainers import (\n",
    "    return_lazy_trainer,\n",
    "    return_optimizer,\n",
    "    return_savehook,\n",
    "    return_schedulerhook,\n",
    ")\n",
    "\n",
    "from deepdisc.model.models import return_lazy_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgfile = '../configs/solo/demo_r50_btk.py'          # The config file which contains information about the model architecture\n",
    "cfg = LazyConfig.load(cfgfile)                       # Load in the config\n",
    "model = return_lazy_model(cfg,freeze=False)          # Build the model from the config specifications\n",
    "cfg.optimizer.params.model = model                   # Set up the training optimizer\n",
    "optimizer = return_optimizer(cfg)\n",
    "\n",
    "\n",
    "cfg.OUTPUT_DIR = output_dir                          #Set the output directory\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4                         #Set the batch size\n",
    "\n",
    "loader = loaders.return_train_loader(cfg, mapper)    # Set up the loader, which formats the data to be fed into the model\n",
    "\n",
    "schedulerHook = return_schedulerhook(optimizer)             # Create a hook to set up the scheduler to control learning rates\n",
    "saveHook = return_savehook(\"model_temp\",save_period=100)    # Create a hook to save the model every 100 iterations\n",
    "hookList = [saveHook, schedulerHook]                 \n",
    "\n",
    "#Initialize the model weights from a pre-trained model\n",
    "#Change this if you want to use a different pre-trained model\n",
    "#cfg.train.init_checkpoint = \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97061ed5-c531-408f-8bf2-3bc4cda8cc95",
   "metadata": {},
   "source": [
    "### Training may take a few minutes  \n",
    "\n",
    "There's only a few objects per image, so we have to train for several iterations to see good results (even for the demo)\n",
    "\n",
    "Training times will be increased if using a Swin transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-guatemala",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = return_lazy_trainer(model, loader, optimizer, cfg, hookList)\n",
    "trainer.set_period(100)  # print loss every 100 iterations\n",
    "trainer.train(0, 1000)   # train for 1000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-illinois",
   "metadata": {},
   "source": [
    "### Plot The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "ax.plot(trainer.lossList, label=r\"$L_{\\rm{tot}}$\")\n",
    "# ax.plot(losses, label=r'$L_{\\rm{tot}}$')\n",
    "ax.set_ylim(0,10)\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"training epoch\", fontsize=20)\n",
    "ax.set_ylabel(\"loss\", fontsize=20)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-inspection",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757e219",
   "metadata": {},
   "source": [
    "We can use the same config to load the model after training.  Just change the path to the trained model weights.  We create a predictor class to feed in the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdisc.inference.predictors import return_predictor\n",
    "\n",
    "\n",
    "cfgfile = '../configs/solo/demo_r50_btk.py'\n",
    "cfg = LazyConfig.load(cfgfile)\n",
    "cfg.OUTPUT_DIR = output_dir\n",
    "cfg.train.init_checkpoint = os.path.join(cfg.OUTPUT_DIR, \"model_temp.pth\")\n",
    "\n",
    "\n",
    "\n",
    "#change these to play with the detection sensitivity\n",
    "cfg.model.roi_heads.box_predictor.test_score_thresh = 0.5\n",
    "cfg.model.roi_heads.box_predictor.test_nms_thresh = 0.3\n",
    "\n",
    "\n",
    "#if using a swin model with cascade ROI heads, change these to to play with the detection sensitivity\n",
    "#for box_predictor in cfg.model.roi_heads.box_predictors:\n",
    "#    box_predictor.test_score_thresh = 0.5\n",
    "#    box_predictor.test_nms_thresh = 0.3\n",
    "\n",
    "\n",
    "predictor = return_predictor(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbb424-204c-418a-888e-3e67568b6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "nsample = 1\n",
    "fig = plt.figure(figsize=(10, 5 * nsample))\n",
    "\n",
    "for i, d in enumerate(random.sample(dataset_dicts[\"test\"], nsample)):\n",
    "    img = reader(key_mapper(d))\n",
    "    print(\"total instances:\", len(d[\"annotations\"]))\n",
    "    v0 = Visualizer(\n",
    "        img,\n",
    "        metadata=astrotest_metadata,\n",
    "        scale=5,\n",
    "        instance_mode=ColorMode.SEGMENTATION,\n",
    "    )\n",
    "    groundTruth = v0.draw_dataset_dict(d)\n",
    "\n",
    "    ax1 = plt.subplot(nsample, 2, 2 * i + 1)\n",
    "    ax1.imshow(groundTruth.get_image())\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title('Ground Truth',fontsize=15)\n",
    "\n",
    "\n",
    "    v1 = Visualizer(\n",
    "        img,\n",
    "        metadata=astrotest_metadata,\n",
    "        scale=5,\n",
    "        instance_mode=ColorMode.SEGMENTATION,\n",
    "    )\n",
    "    outputs = predictor(img)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    out = v1.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    print(\"detected instances:\", len(outputs[\"instances\"].pred_boxes))\n",
    "    print(\"\")\n",
    "    ax1 = plt.subplot(nsample, 2, 2 * i + 2)\n",
    "    ax1.imshow(out.get_image())\n",
    "    ax1.axis(\"off\")\n",
    "    ax1.set_title('Predictions',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85480ae3-0252-4f87-be5e-0ad792bf46b5",
   "metadata": {},
   "source": [
    "This demo is just to show how to set up the training.  We encourage you to add object classes, try different contrast scalings, and train for longer!  \n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "  You can also look at the content of the output below  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5588d43-cfbb-40f5-872d-db9f1935bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['instances'].get_fields().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe91f22-fbd6-4623-a41f-07976b1eefe4",
   "metadata": {},
   "source": [
    "Try to rerun with a different model.  You can do this by using the config in `/configs/solo/demo_swin_btk.py`  \n",
    "\n",
    "If you want to use a pre-trained model as a starting point with this config, you will need to download it here https://dl.fbaipublicfiles.com/detectron2/ViTDet/COCO/cascade_mask_rcnn_swin_b_in21k/f342979038/model_final_246a82.pkl  \n",
    "\n",
    "You will need to change the `cfg.train.init_checkpoint` to the path of the downloaded pre-trained model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93aa662-4e42-4fc8-97ec-78a6c1777039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-btknv]",
   "language": "python",
   "name": "conda-env-.conda-btknv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
